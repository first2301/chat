services:
  # Ollama 서버
  ollama:
    image: ollama/ollama:latest
    container_name: rnd_ollama
    command: serve
    environment:
      - OLLAMA_HOST=0.0.0.0
    ports:
      - "11435:11434"
    networks:
      - rag-network
    volumes:
      - ./app/ollama_server:/root/.ollama
      - ./app/ai_models:/root/.ollama/ai_models
      - ./app/ollama_server/Modelfile:/root/.ollama/Modelfile
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10

  qdrant:
    image: qdrant/qdrant:latest
    container_name: rnd_qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    networks:
      - rag-network
    volumes:
      - ./app/vector_data/qdrant_storage:/qdrant/storage       # 네임드 볼륨 권장
      - ./app/vector_data/snapshot_backup:/qdrant/snapshots
    environment:
      - QDRANT__TELEMETRY_DISABLED=true
      - QDRANT__SERVICE__ENABLE_TLS=false

  backend:
    build:
      context: .
      dockerfile: Dockerfile.cpu
    container_name: rag-backend
    environment:
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_COLLECTION=rag_collection
      - DATA_DIR=/app/data
      - EMBEDDING_MODEL_NAME=/app/embedding_models/BGE-m3-ko
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=ko-llama-8B
    volumes:
      - ./app/backend:/app/backend
      - ./app/embedding_models:/app/embedding_models:ro
      - ./app/data:/app/data:ro
    networks:
      - rag-network
    ports:
      - "8000:8000"
    depends_on:
      - ollama
      - qdrant

networks:
  rag-network:
    driver: bridge
