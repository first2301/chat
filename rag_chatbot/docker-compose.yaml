version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    profiles: ["gpu"]

  rag-gpu:
    build:
      context: ..\..\  # project root
      dockerfile: test1_1/RAG/Dockerfile.gpu
    container_name: rag-gpu
    environment:
      VECTOR_STORE_PATH: /data/vector_db
      EMBEDDING_MODEL_NAME: /models/embedding/BGE-m3-ko
      EMBEDDING_DEVICE: cuda
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: midm-Q8
    volumes:
      - ../RAG/vector_db:/data/vector_db
      - ../../embedding_models:/models/embedding
    ports:
      - "8000:8000"
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    profiles: ["gpu"]

  rag-cpu:
    build:
      context: ..\..\
      dockerfile: test1_1/RAG/Dockerfile.cpu
    container_name: rag-cpu
    environment:
      VECTOR_STORE_PATH: /data/vector_db
      EMBEDDING_MODEL_NAME: /models/embedding/BGE-m3-ko
      EMBEDDING_DEVICE: cpu
      OLLAMA_BASE_URL: http://localhost:11434
      OLLAMA_MODEL: midm-Q8
    volumes:
      - ../RAG/vector_db:/data/vector_db
      - ../../embedding_models:/models/embedding
    ports:
      - "8000:8000"
    profiles: ["cpu"]

